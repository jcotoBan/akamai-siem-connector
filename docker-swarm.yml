version: "3"

services:
# Consumer that generated mock events. Uncomment to use it.
  consumer:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-consumer:${BUILD_VERSION}
#    volumes:
#      - ${HOME_DIR}/base-consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
    depends_on:
      - scheduler
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Consumer that generated mock events. Uncomment to use it.
#  consumer:
#    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-consumer:${BUILD_VERSION}
#    volumes:
#      - ~/.edgerc:/home/consumer/.edgerc
#      - ${HOME_DIR}/consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
#    depends_on:
#      - scheduler
#    deploy:
#      mode: replicated
#      replicas: 1
#      placement:
#        constraints:
#          - node.role == manager

# Processor that stores events in files. Uncomment to use it.
#  processor:
#    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-processor:${BUILD_VERSION}
#    volumes:
#      - ${HOME_DIR}/base-processor/etc/processor.conf:/home/processor/etc/processor.conf
#    depends_on:
#      - scheduler
#    deploy:
#      mode: replicated
#      replicas: 1
#      placement:
#        constraints:
#          - node.role == manager

# Processor that stores events in Apache kafka. Uncomment to use it.
  processor:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-processor-kafka:${BUILD_VERSION}
#    volumes:
#      - ${HOME_DIR}/processor-kafka/etc/processor.conf:/home/processor/etc/processor.conf
    depends_on:
      - scheduler
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Scheduler that defines the jobs for the consumer.
  scheduler:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-scheduler:${BUILD_VERSION}
#    ports:
#      - "1883:1883"
#    volumes:
#      - ${HOME_DIR}/scheduler/etc/scheduler.conf:/home/scheduler/etc/scheduler.conf
#      - ${HOME_DIR}/scheduler/etc/mosquitto.conf:/home/schedyler/etc/mosquitto.conf
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Zookeeper image used to control the broker instances.
  zookeeper:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/zookeeper:${BUILD_VERSION}
#    ports:
#      - "2181:2181"
#      - "8080:8080"
#    volumes:
#      - ${HOME_DIR}/kafka/zookeeper/etc/zookeeper.properties:/home/kafka/etc/zookeeper.properties
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Broker image used to receive the events collected.
  broker:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/kafka-broker:${BUILD_VERSION}
#    ports:
#      - "9092:9092"
#    volumes:
#      - ${HOME_DIR}/kafka/broker/etc/server.properties:/home/kafka/etc/server.properties
    depends_on:
      - zookeeper
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# UI for Apache Kafka.
  broker-ui:
    image: provectuslabs/kafka-ui:latest
    environment:
      - KAFKA_CLUSTERS_0_NAME=broker
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
#    ports:
#      - "8080:8080"
    depends_on:
      - broker
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Proxy for delivery https services.
  ingress:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-connector-ingress:${BUILD_VERSION}
    ports:
      - "443:443"
#    volumes:
#      - ${HOME_DIR}/ingress/etc/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
#      - ${HOME_DIR}/ingress/etc/ssl/nginx-selfsigned.crt:/etc/ssl/certs/nginx-selfsigned.crt
#      - ${HOME_DIR}/ingress/etc/ssl/nginx-selfsigned.crt:/etc/ssl/private/nginx-selfsigned.key
    depends_on:
      - broker-ui
      - dashboards
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Logstash image used to collect the events from kafka.
  logstash:
    image: opensearchproject/logstash-oss-with-opensearch-output-plugin
    volumes:
      - ${HOME_DIR}/logstash/etc/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - broker
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# Opensearch database to store the collected events.
  opensearch:
    image: opensearchproject/opensearch:2.1.0
    environment:
      - discovery.type=single-node
      - node.name=opensearch
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms4096m -Xmx4096m"
#    ports:
#      - "9200:9200/tcp"
#      - "9600:9600/tcp"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker

  # Opensearch dashboards image used to view/filter the collected events.
  dashboards:
    image: opensearchproject/opensearch-dashboards:2.1.0
#    ports:
#      - "5601:5601"
    environment:
      - OPENSEARCH_HOSTS=["https://opensearch:9200"]
    depends_on:
      - opensearch
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker