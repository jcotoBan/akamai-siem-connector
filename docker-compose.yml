version: "3"

services:
  # Base consumer image.
  base-consumer:
    build: base-consumer
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-consumer:${BUILD_VERSION}
    profiles:
      - build

  # Consumer that generated mock events. Uncomment to use it.
  consumer:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-consumer:${BUILD_VERSION}
    container_name: consumer
    hostname: consumer
#    volumes:
#      - ./base-consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
    profiles:
      - run

   # Consumer that collects from Akamai SIEM API. Uncomment to use it.
#  consumer:
#    build:
#      context: ./consumer
#      args:
#        - REPOSITORY_URL=${REPOSITORY_URL}
#        - REPOSITORY_ID=${REPOSITORY_ID}
#        - BUILD_VERSION=${BUILD_VERSION}
#    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-consumer:${BUILD_VERSION}
#    container_name: consumer
#    hostname: consumer
#    volumes:
#      - ~/.edgerc:/home/consumer/.edgerc
#      - ./consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
#    profiles:
#      - build
#      - run

  # Base processor image.
  base-processor:
    build: base-processor
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-processor:${BUILD_VERSION}
    profiles:
      - build

  # Processor that stores events in files. Uncomment to use it.
  processor:
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-processor:${BUILD_VERSION}
    container_name: processor
    hostname: processor
#    volumes:
#      - ./base-processor/etc/processor.conf:/home/processor/etc/processor.conf
    profiles:
      - run

   # Processor that stores events in kafka. Uncomment to use it.
#  processor:
#    build:
#      context: processor-kafka
#      args:
#        - REPOSITORY_URL=${REPOSITORY_URL}
#        - REPOSITORY_ID=${REPOSITORY_ID}
#        - BUILD_VERSION=${BUILD_VERSION}
#    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-processor-s3:${BUILD_VERSION}
#    container_name: processor
#    hostname: processor
#    volumes:
#      - ./processor-kafka/etc/processor.conf:/home/processor/etc/processor.conf
#    profiles:
#      - build
#      - run

  # Scheduler that defines the jobs for the consumer.
  scheduler:
    build: ./scheduler
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-scheduler:${BUILD_VERSION}
    container_name: scheduler
    hostname: scheduler
#    ports:
#      - "1883:1883"
#    volumes:
#      - ./scheduler/etc/scheduler.conf:/home/scheduler/etc/scheduler.conf
#      - ./scheduler/etc/mosquitto.conf:/home/schedyler/etc/mosquitto.conf
    profiles:
      - build
      - run

  # Base kafka image to be used by the broker and zookeeper.
  base-kafka:
    build: ./base-kafka
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-kafka:${BUILD_VERSION}
    profiles:
      - build

  # Zookeeper image used to control the broker instances.
  zookeeper:
    build:
      context: ./kafka/zookeeper
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/zookeeper:${BUILD_VERSION}
    container_name: zookeeper
    hostname: zookeeper
#    ports:
#      - "2181:2181"
#      - "8080:8080"
#    volumes:
#      - ./kafka/zookeeper/etc/zookeeper.properties:/home/kafka/etc/zookeeper.properties
    profiles:
      - run
      - build

  # Broker image used to receive the events collected.
  broker:
    build:
      context: ./kafka/broker
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/kafka-broker:${BUILD_VERSION}
    container_name: broker
    hostname: broker
#    ports:
#      - "9092:9092"
#    volumes:
#      - ./kafka/broker/etc/server.properties:/home/kafka/etc/server.properties
    depends_on:
      - zookeeper
    profiles:
      - run
      - build

  # Elasticsearch image used to store the events.
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: elasticsearch
    hostname: elasticsearch
    environment:
      - discovery.type=single-node
      - node.name=elasticsearch
#    ports:
#      - "9200:9200/tcp"
#      - "9300:9300/tcp"
    stdin_open: true
    tty: true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    profiles:
      - run

  # Logstash image used to collect the events from kafka and store in elasticsearch.
  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.0
    container_name: logstash
    hostname: logstash
    stdin_open: true
    tty: true
    volumes:
      - ./elk/logstash/etc/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch
      - broker
    profiles:
      - run

  # Kibana image used to view/filter. the collected events.
  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.0
    container_name: kibana
    hostname: kibana
    environment:
      - elasticsearch.hosts=http://elasticsearch:9200
      - server.host=localhost
    stdin_open: true
    ports:
      - "5601:5601/tcp"
    depends_on:
      - elasticsearch
    profiles:
      - run