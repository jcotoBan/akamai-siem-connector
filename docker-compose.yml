version: "3"

services:
# Base Apache kafka image to be used to construct the broker and zookeeper.
  base-kafka:
    build: base-kafka
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-kafka:${BUILD_VERSION}
    profiles:
      - build

# Base consumer image used to construct Akamai SIEM consumer. It also generates mock events.
  base-consumer:
    build: base-consumer
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-consumer:${BUILD_VERSION}
    environment:
      - NODE_OPTIONS="--max-old-space-size=1024"
    container_name: base-consumer
    hostname: base-consumer
    volumes:
#      - ${HOME_DIR}/base-consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
      - consumer-logs:/home/consumer/logs
    depends_on:
      - scheduler
    profiles:
      - build
      - run

# Base processor image used to constructor processors to consume Akamai SIEM events.
  base-processor:
    build: base-processor
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/base-akamai-siem-processor:${BUILD_VERSION}
    environment:
      - NODE_OPTIONS="--max-old-space-size=1024"
    container_name: base-processor
    hostname: base-processor
    volumes:
#      - ${HOME_DIR}/base-processor/etc/processor.conf:/home/processor/etc/processor.conf
      - processor-logs:/home/processor/logs
      - processor-data:/home/processor/data
    depends_on:
      - scheduler
    profiles:
      - build
      - run

# Consumer that collects from Akamai SIEM.
  consumer:
    build:
      context: consumer
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-consumer:${BUILD_VERSION}
#    environment:
#      - NODE_OPTIONS="--max-old-space-size=1024"
#    container_name: consumer
#    hostname: consumer
#    volumes:
#      - ~/.edgerc:/home/consumer/.edgerc
#      - ${HOME_DIR}/consumer/etc/consumer.conf:/home/consumer/etc/consumer.conf
#      - consumer-logs:/home/consumer/logs
#    depends_on:
#      - scheduler
    profiles:
#      - run
      - build

# Processor that stores events in Apache kafka Broker.
  processor-kafka:
    build:
      context: processor-kafka
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-processor-kafka:${BUILD_VERSION}
#    environment:
#      - NODE_OPTIONS="--max-old-space-size=1024"
#    container_name: processor-kafka
#    hostname: processor-kafka
#    volumes:
#      - ${HOME_DIR}/processor-kafka/etc/processor.conf:/home/processor/etc/processor.conf
#      - processor-logs:/home/processor/logs
#      - processor-data:/home/processor/data
#    depends_on:
#      - scheduler
    profiles:
#      - run
      - build

# Scheduler that defines the jobs for the consumer.
  scheduler:
    build: scheduler
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-scheduler:${BUILD_VERSION}
    container_name: scheduler
    hostname: scheduler
#    ports:
#      - "1883:1883"
    volumes:
#      - ${HOME_DIR}/scheduler/etc/scheduler.conf:/home/scheduler/etc/scheduler.conf
#      - ${HOME_DIR}/scheduler/etc/mosquitto.conf:/home/schedyler/etc/mosquitto.conf
      - scheduler-logs:/home/scheduler/logs
      - scheduler-data:/home/scheduler/data
    profiles:
      - build
      - run

# Zookeeper image used to control the broker instances.
  zookeeper:
    build:
      context: kafka/zookeeper
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/zookeeper:${BUILD_VERSION}
    container_name: zookeeper
    hostname: zookeeper
#    ports:
#      - "2181:2181"
#      - "8080:8080"
    volumes:
#      - ${HOME_DIR}/kafka/zookeeper/etc/zookeeper.properties:/home/kafka/etc/zookeeper.properties
      - zookeeper-logs:/home/kafka/logs
      - zookeeper-data:/home/kafka/data
    profiles:
      - run
      - build

# Apache Kafka Broker image used to receive the events collected.
  kafka-broker:
    build:
      context: kafka/broker
      args:
        - REPOSITORY_URL=${REPOSITORY_URL}
        - REPOSITORY_ID=${REPOSITORY_ID}
        - BUILD_VERSION=${BUILD_VERSION}
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/kafka-broker:${BUILD_VERSION}
    container_name: kafka-broker
    hostname: kafka-broker
#    ports:
#      - "9092:9092"
    volumes:
#      - ${HOME_DIR}/kafka/broker/etc/server.properties:/home/kafka/etc/server.properties
      - kafka-broker-logs:/home/kafka/logs
      - kafka-broker-data:/home/kafka/data
    depends_on:
      - zookeeper
    profiles:
      - run
      - build

# UI for Apache Kafka Broker.
  kafka-broker-ui:
    image: provectuslabs/kafka-ui:latest
    environment:
      - KAFKA_CLUSTERS_0_NAME=kafka-broker
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka-broker:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
      - SERVER_SERVLET_CONTEXT_PATH=
    container_name: kafka-broker-ui
    hostname: kafka-broker-ui
#    ports:
#      - "8080:8080"
    depends_on:
      - kafka-broker
    profiles:
      - run

# Ingress (HTTPs delivery).
  ingress:
    build: ingress
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-ingress:${BUILD_VERSION}
    container_name: ingress
    hostname: ingress
    ports:
      - "443:443"
    volumes:
#      - ${HOME_DIR}/ingress/etc/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
#      - ${HOME_DIR}/ingress/etc/ssl/nginx-selfsigned.crt:/etc/ssl/certs/nginx-selfsigned.crt
#      - ${HOME_DIR}/ingress/etc/ssl/nginx-selfsigned.crt:/etc/ssl/private/nginx-selfsigned.key
      - ingress-logs:/var/log/nginx
    depends_on:
      - kafka-broker-ui
      - opensearch-dashboards
    profiles:
      - run
      - build

# Logstash image used to collect the events from Apache Kafka Broker and store into opensearch. Use this only for
# troubleshooting or testing.
  logstash-opensearch:
    image: opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.3
    environment:
      - "LS_JAVA_OPTS=-Xms1024m -Xmx1024m"
    container_name: logstash-opensearch
    hostname: logstash-opensearch
    volumes:
      - ${HOME_DIR}/logstash-opensearch/etc/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - kafka-broker
      - opensearch
    profiles:
      - run

  # Logstash image used to collect the events from Apache Kafka Broker and store into Microsoft Sentinel.
  logstash-mssentinel:
    build: logstash-mssentinel
    image: ${REPOSITORY_URL}/${REPOSITORY_ID}/akamai-siem-logstash-mssentinel:${BUILD_VERSION}
#    environment:
#      - "LS_JAVA_OPTS=-Xms1024m -Xmx1024m"
#    container_name: logstash-mssentinel
#    hostname: logstash-mssentinel
#    volumes:
#      - ${HOME_DIR}/logstash-mssentinel/etc/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
#    depends_on:
#      - kafka-broker
    profiles:
#      - run
      - build

# Opensearch database to store the collected events. Use this only for troubleshooting or testing.
  opensearch:
    image: opensearchproject/opensearch:2.1.0
    environment:
      - discovery.type=single-node
      - node.name=opensearch
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms1024m -Xmx1024m"
    container_name: opensearch
    hostname: opensearch
#    ports:
#      - "9200:9200/tcp"
#      - "9600:9600/tcp"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    profiles:
      - run

# Opensearch dashboards image used to view/filter the collected events. Use this only for troubleshooting or testing.
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.1.0
    container_name: opensearch-dashboards
    hostname: opensearch-dashboards
    environment:
      - OPENSEARCH_HOSTS=["https://opensearch:9200"]
#    ports:
#      - "5601:5601"
    depends_on:
      - opensearch
    profiles:
      - run

volumes:
  consumer-logs:
  processor-logs:
  processor-data:
  scheduler-logs:
  scheduler-data:
  ingress-logs:
  kafka-broker-logs:
  kafka-broker-data:
  zookeeper-logs:
  zookeeper-data: